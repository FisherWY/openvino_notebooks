{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Text Image Super Resolution with OpenVINO\n",
    "\n",
    "Super Resolution is the process of enhancing the quality of an image by increasing the pixel count using deep learning. This notebook shows the Text Image Super Resolution (TISR) which takes just one low resolution image with blured text, and output a high resolution image which have clear text. We use a model called [text-image-super-resolution-0001](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/intel/text-image-super-resolution-0001/README.md) which is available from the Open Model Zoo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation\n",
    "\n",
    "### First let's import the packages we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import load_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the model we need\n",
    "\n",
    "We use `omz_downloader` to download the model from open model zoo. The models in open model zoo are already converted to OpenVINO Intermediate Representation (IR), so there is no need to use `omz_converter`.  \n",
    "\n",
    "In this cell, we set the device type, model path, model name and download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to use for inference. For example, \"CPU\", or \"GPU\"\n",
    "DEVICE = \"CPU\"\n",
    "# Directory where model will be downloaded\n",
    "DOWNLOAD_DIR = \"downloads\"\n",
    "# Model name\n",
    "MODEL_NAME = \"text-image-super-resolution-0001\"\n",
    "# Model precision\n",
    "PRECISION = \"FP32\"\n",
    "# OMZ_downloader command\n",
    "download_command = f\"omz_downloader \" \\\n",
    "                   f\"--name {MODEL_NAME} \" \\\n",
    "                   f\"--output_dir {DOWNLOAD_DIR} \" \\\n",
    "                   f\"--cache_dir {DOWNLOAD_DIR}\"\n",
    "# Execute download command\n",
    "! $download_command\n",
    "\n",
    "# Model file path\n",
    "MODEL_FILE = f\"{DOWNLOAD_DIR}/intel/{MODEL_NAME}/{PRECISION}/{MODEL_NAME}.xml\"\n",
    "model_xml_path = Path(MODEL_FILE).with_suffix(\".xml\")\n",
    "\n",
    "# print(f\"Model name: {model_name}\")\n",
    "# print(f\"Model XML file path: {model_xml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Text Image Super Resolution Model\n",
    "\n",
    "### After preparation, now we are going to load the model\n",
    "\n",
    "The Text Image Super Resolution model expects one input: the input shape format as (B, C, H, W), B is batch-size, C is number of channels, H is image height, W is image width. By default the input shape is (1, 1, 360, 640).\n",
    "\n",
    "Load the model in Inference Engine with ie.read_model, compile it for the specified device with ie.compile_model, and get information about the network inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "model = ie.read_model(model=model_xml_path)\n",
    "compiled_model = ie.compile_model(model=model, device_name=DEVICE)\n",
    "\n",
    "input_layer_ir = compiled_model.input(0)\n",
    "output_layer_ir = compiled_model.output(0)\n",
    "\n",
    "# Get the expected input and target shape.\n",
    "input_height, input_width = list(input_layer_ir.shape)[2:]\n",
    "target_height, target_width = list(output_layer_ir.shape)[2:]\n",
    "\n",
    "upsample_factor = int(target_height / input_height)\n",
    "\n",
    "print(f\"The network expects inputs with a width of {input_width}, \" f\"height of {input_height}\")\n",
    "print(f\"The network returns images with a width of {target_width}, \" f\"height of {target_height}\")\n",
    "\n",
    "print(\n",
    "    f\"The image sides are upsampled by a factor {upsample_factor}. \"\n",
    "    f\"The new image is {upsample_factor**2} times as large as the \"\n",
    "    \"original image\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Image And Prepare For Inference\n",
    "\n",
    "Here we define some functions which will be used to convert image and visualization.\n",
    "Then we load the source image, convert to gray scale and reshape it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgr_to_gray(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert BGR image to Gray scale image\n",
    "    \n",
    "    :param image: an BGR image store as numpy array\n",
    "    :return: gray scale image as numpy array\n",
    "    \"\"\"\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "def viz_result_image(\n",
    "    result_image: np.ndarray,\n",
    "    source_image: np.ndarray = None,\n",
    "    result_title: str = None,\n",
    "    source_title: str = None,\n",
    "    result_gray_scale: bool = False,\n",
    "    source_gray_scale: bool = False\n",
    ") -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    Show result image, optionally together with source images. Modify from notebook_utils.viz_result_image, add gray scale image support.\n",
    "\n",
    "    :param result_image: Numpy array of result image.\n",
    "    :param source_image: Numpy array of source image. If provided this image will be shown next to the result image.\n",
    "    :param result_title: Title to display for the result image.\n",
    "    :param source_title: Title to display for the source image.\n",
    "    :param result_gray_scale: If true, matplotlib will plot the image in gray scale.\n",
    "    :param source_gray_scale: If true, matplotlib will plot the image in gray scale.\n",
    "    :return: Matplotlib figure with result image\n",
    "    \"\"\"\n",
    "    num_images = 1 if source_image is None else 2\n",
    "    fig, ax = plt.subplots(1, num_images, figsize=(16, 8), squeeze=False)\n",
    "    if source_image is not None:\n",
    "        ax[0, 0].imshow(source_image, cmap='gray' if source_gray_scale else None)\n",
    "        ax[0, 0].set_title(source_title)\n",
    "    ax[0, num_images - 1].imshow(result_image, cmap='gray' if result_gray_scale else None)\n",
    "    ax[0, num_images - 1].set_title(result_title)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def convert_result_to_image(result) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert network result of floating point numbers to image with integer\n",
    "    values from 0-255. Values outside this range are clipped to 0 and 255.\n",
    "\n",
    "    :param result: a single superresolution network result in N,C,H,W shape\n",
    "    \"\"\"\n",
    "    result = result.squeeze(0).transpose(1, 2, 0)\n",
    "    result *= 255\n",
    "    result[result < 0] = 0\n",
    "    result[result > 255] = 255\n",
    "    result = result.astype(np.uint8)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Read source image\n",
    "img = load_image(\"data/text.png\")\n",
    "# Convert source image to gray scale\n",
    "gray_img = bgr_to_gray(img)\n",
    "\n",
    "# Resize\n",
    "resized_img = cv2.resize(gray_img, (input_width, input_height))\n",
    "\n",
    "# Reshape to fit the model's input shape\n",
    "input_img = np.expand_dims(resized_img, axis=0)\n",
    "input_img = np.expand_dims(input_img, axis=0)\n",
    "\n",
    "print(f\"Source image shape: {img.shape}\")\n",
    "print(f\"Gray scale image shape: {gray_img.shape}\")\n",
    "print(f\"Gray scale image resized shape: {resized_img.shape}\")\n",
    "print(f\"Input image shape: {input_img.shape}\")\n",
    "# View the source image and gray scale image\n",
    "# viz_result_image(gray_img, img,\n",
    "#                  source_title=\"Source image\", result_title=\"Gray image\",\n",
    "#                  source_gray_scale=False, result_gray_scale=True)\n",
    "# View the gray scale image and resized image\n",
    "viz_result_image(resized_img, gray_img,\n",
    "                 result_title=\"Resized gray image\", source_title=\"Gray image\",\n",
    "                 result_gray_scale=True, source_gray_scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Do Inference And Plot The Result\n",
    "\n",
    "### Do inference using compiled model, squeeze the shape and compare to source image\n",
    "\n",
    "The model's output shape is (1, 1, 1080, 1920), we need to reduce the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data to the model and get result\n",
    "result = compiled_model([input_img])[output_layer_ir]\n",
    "# Reduce the dimension\n",
    "superresolution_image = convert_result_to_image(result)\n",
    "print(f\"Model's output shape: {result.shape}\")\n",
    "print(f\"Output image shape: {superresolution_image.shape}\")\n",
    "\n",
    "viz_result_image(superresolution_image, img,\n",
    "                 result_title=\"Super resolution image\", source_title=\"Source image\",\n",
    "                 result_gray_scale=True, source_gray_scale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to bicubic image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicubic_image = cv2.resize(gray_img, (target_width, target_height), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "viz_result_image(superresolution_image, bicubic_image,\n",
    "                 result_title=\"Super resolution image\", source_title=\"Bicubic image\",\n",
    "                 result_gray_scale=True, source_gray_scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Output To File\n",
    "\n",
    "Save the super resolution image and bicubic image to the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Output path\n",
    "OUTPUT_PATH = \"output\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Save the images\n",
    "cv2.imwrite(f\"{OUTPUT_PATH}/superresolution_image.png\", superresolution_image)\n",
    "cv2.imwrite(f\"{OUTPUT_PATH}/bicubic_image.png\", bicubic_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
